# Image Classification with PyTorch â€” MLP Variants (Dropout & BatchNorm)
**Notebook:** `ImageClassificationwithPytorch.ipynb`  
~ By Guardians of the Galaxy

This project trains several **fully-connected (MLP)** image classifiers in PyTorch on a custom **10â€‘class** dataset. It includes data normalization computed from the training set, multiple model variants (**Baseline**, **Dropout**, **BatchNorm**, **Dropout+BatchNorm**), early stopping, checkpointing, and endâ€‘toâ€‘end evaluation.

---

## ðŸŽ¯ Objectives
- Load and prepare an image dataset from a ZIP archive, compute **dataset mean/std** for normalization.
- Implement and compare multiple **MLP architectures** (no convolutions) with **Dropout** and **BatchNorm** variants.
- Train on GPU if available; track **loss** per epoch and evaluate on a heldâ€‘out test set.
- Report **accuracy** (and precision/recall where computed) and save the best model checkpoint.

---

## ðŸ§ª Data
- **Size:** ~**4,844** images (from notebook output).  
- **Classes:** **10** (IDs 0â€“9; class distribution printed in notebook).  
- **Normalization:** computed mean/std used in `transforms.Normalize(...)`:
  - Mean: `tensor([0.6763, 0.5651, 0.4429])`
  - Std: `tensor([0.2441, 0.2925, 0.3333])`
- **Transforms:** `Resize` â†’ `ToTensor` â†’ `Normalize` (no heavy augmentation in the current run).

> The notebook unzips the dataset and expects a directory layout suitable for loading images (custom loader, not `ImageFolder`), then builds PyTorch `DataLoader`s.

---

## ðŸ§  Models
Custom `nn.Module` variants implemented:
- **BaselineNN** â€” plain MLP
- **Model1_Dropout** â€” adds `nn.Dropout`
- **Model3_BatchNorm** / **BatchNormNN** â€” adds `nn.BatchNorm1d`
- **Model4_Dropout_L2** â€” combined Dropout + (L2 if enabled in loss/optimizer)

> Layers used: `nn.Linear`, `nn.ReLU`, optional `nn.Dropout`, `nn.BatchNorm1d`  
> **No** `nn.Conv2d` is used; images are flattened into vectors.

---

## âš™ï¸ Training Setup
- **Device:** CUDA used when available
- **Loss:** `nn.CrossEntropyLoss()`
- **Optimizer:** `Adam (lr=0.001)`
- **Batch size:** `32` (with `num_workers=2`)
- **Epochs:** set to `10` (early stopping triggered around epoch 9 in one run)
- **Checkpointing:** best model saved via `torch.save(...)`
- **(Optional)** Early stopping logic present (triggered in runs shown)

---

## ðŸ“ˆ Results (from notebook run)
Best observed metrics from outputs:
- **Test Accuracy:** **0.999**
- **Test Precision:** **0.999**
- **Test Recall:** **0.999**

Training/validation losses are printed per epoch (see notebook for full logs). Results vary by model variant; BatchNorm/Dropout variants tend to converge faster and achieve the top scores in the provided runs.


---

## ðŸ“Š Visualizations & Reports
- **Perâ€‘epoch**: Train/Test (validation) **loss**
- (Recommended) Add **confusion matrix** and `classification_report` to analyze perâ€‘class performance; hooks exist for accuracy/precision/recall in code.

---

## ðŸ“ Repository Structure
```text
â”œâ”€â”€ ImageClassificationwithPytorch.ipynb   # Main notebook
â””â”€â”€ README.md         # This file
```

If you export artifacts (best model) or figures, consider adding:
```text
â”œâ”€â”€ checkpoints/      # Saved .pt/.pth files
â””â”€â”€ images/           # Saved plots (loss curves, confusion matrix)
```

---

## ðŸš€ Setup & Run
**Dependencies:** `torch`, `torchvision`, `pillow`, `scikit-learn`, `matplotlib`, `pandas`

Install:
```bash
pip install -U torch torchvision torchaudio pillow scikit-learn matplotlib pandas
```

Run:
```bash
jupyter notebook "ImageClassificationwithPytorch.ipynb"
```

> Place (or unzip) your dataset where the notebook expects it. Update the data path variables in the first cells if needed.

---

## ðŸ’¡ Next Steps
- **Data Augmentation:** add `RandomHorizontalFlip`, `RandomRotation`, `ColorJitter`, or `RandomResizedCrop` to improve robustness.
- **Schedulers:** try `StepLR`, `CosineAnnealingLR`, or `OneCycleLR` for better convergence.
- **Regularization:** confirm **L2** via `weight_decay` or explicit penalty in the loss; experiment with dropout rates.
- **Modeling:** compare against a small **CNN** or **pretrained** backbones (e.g., ResNet18) to benchmark MLPs.
- **Evaluation:** log a **confusion matrix**, **perâ€‘class precision/recall/F1**, and **ROC/PR** (oneâ€‘vsâ€‘rest) for deeper insight.
